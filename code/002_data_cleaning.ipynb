{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import pickle\n",
    "import string\n",
    "from langdetect import detect\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "#pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning hurricane tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in all hurricane tweets from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (4,12,18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "hurricanes = pd.read_csv(\"../Data/hurricane_tweets.csv\")\n",
    "floods = pd.read_csv(\"../Data/df_floods.csv\")\n",
    "fires = pd.read_csv(\"../Data/all_fires.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge dataframes into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes together\n",
    "df = pd.concat([hurricanes, floods, fires], sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = \"Unnamed: 0\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in hurricanes[\"text\"]:\n",
    "#     try:\n",
    "#         detect(text)\n",
    "#     except:\n",
    "#         noLang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, valu in onlyText.items():\n",
    "#         try:\n",
    "#             if detect(val[0]) !=\"en\":\n",
    "#                 foreignLangs[key]= val\n",
    "#                 foreignLangs[key].append(detect(val[0]))\n",
    "\n",
    "#         except:\n",
    "#             noLang[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hurricanes[\"language\"] = hurricanes[\"text\"].apply(detect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"text\", \"disaster\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"OFFICALLY TROPICAL STORM DORIAN Where is it Going? Tropical Depression 5 Hurricane Dorian Track 2019 https://youtu.be/SKCqARFvsQw\\xa0 The latest on the STORM'S TRACK!  in the above YOUTUBE LINK!!! @FlyRts @FearRTs @GFXCoach #dorian #florida #hurricane #hurricanedorian #tropicalstormdorianpic.twitter.com/RpMN7ewuLs\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#TDFIVE TO BECOME A #Hurricane THIS WEEK\\n\\nA system, located hundreds of miles from the Lesser Antilles, is expected to become #TropicalStormDorian tomorrow.  It is also forecast to become #HurricaneDorian later this week!  Start preparing now!\\n\\n#apexwx #tropics #Atlantic #stormpic.twitter.com/MsRpq4mRRZ'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[6,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df[\"text\"].str.contains(\"blog\")].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code was adapted from this stackoverflow answer\n",
    "# https://stackoverflow.com/questions/8376691/how-to-remove-hashtag-user-link-of-a-tweet-using-regular-expression\n",
    "def strip_all_entities(text):\n",
    "    entity_prefixes = ['@','#']\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,'')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase text\n",
    "df[\"text\"] = df[\"text\"].str.lower()\n",
    "\n",
    "\n",
    "\n",
    "# remove URLs\n",
    "df['text'] = df['text'].map(lambda x: re.sub('http[s]?:\\/\\/[^\\s]*', ' ', x))\n",
    "\n",
    "\n",
    "# remove URL cutoffs\n",
    "df['text'] = df['text'].map(lambda x: re.sub('\\\\[^\\s]*', ' ', x))\n",
    "\n",
    "\n",
    "\n",
    "# remove spaces\n",
    "df['text'] = df['text'].map(lambda x: re.sub('\\n', ' ', x))\n",
    "\n",
    "\n",
    "# remove picture URLs\n",
    "df['text'] = df['text'].map(lambda x: re.sub('pic.twitter.com\\/[^\\s]*', ' ', x))\n",
    "\n",
    "# remove blog/map type\n",
    "df['text'] = df['text'].map(lambda x: re.sub('blog\\/maps\\/info\\/[^\\s]*', '', x))\n",
    "\n",
    "\n",
    "\n",
    "# remove hashtags and AT users\n",
    "df['text'] = df['text'].apply(strip_all_entities)\n",
    "\n",
    "\n",
    "\n",
    "# remove single quotations\n",
    "df[\"text\"] = df[\"text\"].map(lambda x: re.sub(\"'\", \"\", x))\n",
    "df[\"text\"] = df[\"text\"].map(lambda x: re.sub(\"'\", \"\", x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# remove characters that are not word characters or digits\n",
    "df[\"text\"] = df[\"text\"].map(lambda x: re.sub(\"[^\\w\\d]\", \" \", x))\n",
    "\n",
    "# remove all characters that are not letters\n",
    "df['text'] = df['text'].map(lambda x: re.sub(\"[^a-zA-Z]\", \" \", x))\n",
    "\n",
    "# remove multiple spaces\n",
    "df['text'] = df['text'].map(lambda x: re.sub(\"\\s{2,6}\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        offically tropical storm dorian where is it going tropical depressionhurricane dorian trackthe latest on the storms track in the above youtube link                                          \n",
       "1        tropical storm dorian projected path spaghetti models                                                                                                                                        \n",
       "2        futura tormenta tropical pasando por el sur de puerto rico                                                                                                                                   \n",
       "3        blogmapsinfo                                                                                                                                                                                 \n",
       "4        blogmapsinfo                                                                                                                                                                                 \n",
       "             ...                                                                                                                                                                                      \n",
       "40377    fantastic news on this winter solstice day all mandatory and voluntary evacuation orders are                                                                                                 \n",
       "40378    all evacuation orders lifted all sb county mandatory and voluntary evacuation orders have been lifted effectiveam today                                                                      \n",
       "40379    sb county oem all evacuation orders lifted all sb county mandatory and voluntary evacuation orders have been lifted effectiveam today                                                        \n",
       "40380    rt prime example of teamwork and mutual support within the forestry community                                                                                                                \n",
       "40381    thanks to all the brave firefighters and and everyone else who work so hard to keep us informed and safe we are going home today and so blessed our home is still standing all because of you\n",
       "Name: text, Length: 77722, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tweets with this url type\n",
    "df = df[~df[\"text\"].str.contains(\"blogmapsinfo\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows\n",
    "df.drop_duplicates(subset='text', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].map(lambda x: re.sub(\"\\s{2,6}\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop row with only one space\n",
    "df = df[~(df[\"text\"]== \" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop empty row\n",
    "df = df[~(df[\"text\"]== \"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 62922)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect languages of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code was used to test for errors that would prevent the detect function from running\n",
    "# languages = []\n",
    "# for i in range(101,150):\n",
    "#     try:\n",
    "#         languages.append(detect(df.iloc[i, 0]))\n",
    "#     except:\n",
    "#         print(f\"error in row {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply detect function on text column\n",
    "df[\"languages\"] = df[\"text\"].apply(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62921, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59586, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select for tweets that are English only\n",
    "## this dropped 3_335 rows \n",
    "df_en = df[df[\"languages\"] == \"en\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue cleaning on english column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are removing multiple copies of the same letter. For example \"thanksssssssss\" is updated to \"thanks\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:576: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "# Wrote this as a function but it took forever to run, so breaking it out individually\n",
    "# Saving code for future reference\n",
    "\n",
    "# # list of all English letters\n",
    "# letters = list(string.ascii_lowercase)\n",
    "\n",
    "# # list of letters that typically don't repeat twice in an English word\n",
    "# double_letters = [\"q\", \"u\", \"w\", \"y\"]\n",
    "\n",
    "# def remove_repeats(letters):\n",
    "#     for letter in letters:\n",
    "#         if letter in double_letters:\n",
    "#             df_en[\"text\"].map(lambda x: re.sub(re.escape(letter)+\"{2,10}\", re.escape(letter), x))\n",
    "#         else:\n",
    "#             df_en[\"text\"].map(lambda x: re.sub(re.escape(letter)+\"{3,10}\", re.escape(letter), x))\n",
    "\n",
    "\n",
    "# df_en.loc[:, \"text\"] = df_en[\"text\"].map(remove_repeats)\n",
    "\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"a{3,10}\", \"a\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"b{3,10}\", \"b\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"c{3,10}\", \"c\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"d{3,10}\", \"d\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"e{3,10}\", \"e\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"f{3,10}\", \"f\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"g{3,10}\", \"g\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"h{3,10}\", \"h\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"i{3,10}\", \"i\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"j{3,10}\", \"j\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"k{3,10}\", \"k\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"l{3,10}\", \"l\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"m{3,10}\", \"m\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"n{3,10}\", \"n\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"o{3,10}\", \"o\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"p{3,10}\", \"p\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"q{2,10}\", \"q\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"r{3,10}\", \"r\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"s{3,10}\", \"s\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"t{3,10}\", \"t\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"u{2,10}\", \"u\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"v{3,10}\", \"v\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"x{3,10}\", \"x\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"y{2,10}\", \"y\", x))\n",
    "df_en.loc[:, \"text\"] = df_en['text'].map(lambda x: re.sub(\"z{3,10}\", \"z\", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csf\n",
    "df_en.to_csv(\"../Data/all_tweets_clean2.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
